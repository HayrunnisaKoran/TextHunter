"""
Veri tekrarÄ± kontrolÃ¼ ve temizleme scripti
"""

import pandas as pd
import json
import os
from pathlib import Path

DATA_DIR = Path("../Data/raw")

def check_duplicates_csv(file_path, name):
    """CSV dosyasÄ±nda duplicate kontrolÃ¼ yapar"""
    print(f"\n{'='*60}")
    print(f"{name} - CSV KontrolÃ¼")
    print(f"{'='*60}")
    
    if not file_path.exists():
        print(f"âš  Dosya bulunamadÄ±: {file_path}")
        return None
    
    try:
        df = pd.read_csv(file_path)
        total_rows = len(df)
        
        # Text kolonunda duplicate kontrolÃ¼
        if 'text' in df.columns:
            unique_texts = df['text'].nunique()
            duplicates = df[df.duplicated(subset=['text'], keep=False)]
            duplicate_count = len(duplicates)
            
            print(f"Toplam satÄ±r sayÄ±sÄ± (pandas): {total_rows}")
            print(f"Unique text sayÄ±sÄ±: {unique_texts}")
            print(f"Duplicate veri sayÄ±sÄ±: {duplicate_count}")
            
            if duplicate_count > 0:
                print(f"\nâš  DUPLICATE BULUNDU!")
                print(f"Duplicate Ã¶rnekleri:")
                print(duplicates[['text', 'label']].head(5).to_string())
                return df, duplicates
            else:
                print(f"âœ“ Duplicate YOK - TÃ¼m veriler unique")
                return df, None
        else:
            print(f"âš  'text' kolonu bulunamadÄ±")
            print(f"Kolonlar: {list(df.columns)}")
            return df, None
            
    except Exception as e:
        print(f"âŒ Hata: {e}")
        return None, None

def check_duplicates_json(file_path, name):
    """JSON dosyasÄ±nda duplicate kontrolÃ¼ yapar"""
    print(f"\n{'='*60}")
    print(f"{name} - JSON KontrolÃ¼")
    print(f"{'='*60}")
    
    if not file_path.exists():
        print(f"âš  Dosya bulunamadÄ±: {file_path}")
        return None
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        total_count = len(data)
        texts = [item.get('text', '').strip() for item in data if 'text' in item]
        unique_texts = len(set(texts))
        duplicate_count = total_count - unique_texts
        
        print(f"Toplam veri sayÄ±sÄ±: {total_count}")
        print(f"Unique text sayÄ±sÄ±: {unique_texts}")
        print(f"Duplicate veri sayÄ±sÄ±: {duplicate_count}")
        
        if duplicate_count > 0:
            print(f"\nâš  DUPLICATE BULUNDU!")
            # Duplicate'leri bul
            seen = set()
            duplicates = []
            for item in data:
                text = item.get('text', '').strip()
                if text in seen:
                    duplicates.append(item)
                else:
                    seen.add(text)
            
            print(f"Duplicate Ã¶rnekleri (ilk 3):")
            for i, dup in enumerate(duplicates[:3], 1):
                print(f"\n{i}. Text: {dup.get('text', '')[:100]}...")
                print(f"   Label: {dup.get('label', '')}")
                print(f"   Date: {dup.get('generated_date', '')}")
            
            return data, duplicates
        else:
            print(f"âœ“ Duplicate YOK - TÃ¼m veriler unique")
            return data, None
            
    except Exception as e:
        print(f"âŒ Hata: {e}")
        return None, None

def main():
    print("\n" + "="*60)
    print("VERÄ° TEKRARI (DUPLICATE) KONTROLÃœ")
    print("="*60)
    
    # 1. Human Abstracts CSV
    human_csv = DATA_DIR / "human_abstracts.csv"
    df_human, dup_human = check_duplicates_csv(human_csv, "Human Abstracts")
    
    # 2. AI Abstracts CSV
    ai_csv = DATA_DIR / "ai_abstracts.csv"
    df_ai, dup_ai = check_duplicates_csv(ai_csv, "AI Abstracts")
    
    # 3. Combined Dataset CSV
    combined_csv = DATA_DIR / "combined_dataset.csv"
    df_combined, dup_combined = check_duplicates_csv(combined_csv, "Combined Dataset")
    
    # 4. JSON dosyalarÄ±
    human_json = DATA_DIR / "human_abstracts.json"
    data_human_json, dup_human_json = check_duplicates_json(human_json, "Human Abstracts")
    
    ai_json = DATA_DIR / "ai_abstracts.json"
    data_ai_json, dup_ai_json = check_duplicates_json(ai_json, "AI Abstracts")
    
    # Ã–zet
    print(f"\n{'='*60}")
    print("Ã–ZET")
    print(f"{'='*60}")
    
    has_duplicates = False
    if dup_human is not None and len(dup_human) > 0:
        print(f"âš  Human CSV: {len(dup_human)} duplicate")
        has_duplicates = True
    if dup_ai is not None and len(dup_ai) > 0:
        print(f"âš  AI CSV: {len(dup_ai)} duplicate")
        has_duplicates = True
    if dup_combined is not None and len(dup_combined) > 0:
        print(f"âš  Combined CSV: {len(dup_combined)} duplicate")
        has_duplicates = True
    if dup_human_json is not None and len(dup_human_json) > 0:
        print(f"âš  Human JSON: {len(dup_human_json)} duplicate")
        has_duplicates = True
    if dup_ai_json is not None and len(dup_ai_json) > 0:
        print(f"âš  AI JSON: {len(dup_ai_json)} duplicate")
        has_duplicates = True
    
    if not has_duplicates:
        print("âœ“ TÃ¼m dosyalarda duplicate YOK!")
    else:
        print("\nðŸ’¡ Duplicate temizlemek iÃ§in:")
        print("   python remove_duplicates.py")
    
    print("="*60 + "\n")

if __name__ == "__main__":
    main()

